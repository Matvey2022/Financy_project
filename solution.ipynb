{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start with reading data. I will unzip and convert it to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping and renaming complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# path to my folder\n",
    "zip_folder = \"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\october2023typeB\"\n",
    "output_folder = \"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\Unziped_Data\"\n",
    "\n",
    "# make sure that output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# work with all zip files in folder\n",
    "for zip_filename in os.listdir(zip_folder):\n",
    "    if zip_filename.endswith(\".zip\"):\n",
    "        with ZipFile(os.path.join(zip_folder, zip_filename), 'r') as zip_ref:\n",
    "            # Extract all files to the output folder\n",
    "            zip_ref.extractall(output_folder)\n",
    "\n",
    "#.txt to .csv\n",
    "for file in os.listdir(output_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        txt_path = os.path.join(output_folder, file)\n",
    "        csv_path = txt_path.replace(\".txt\", \".csv\")\n",
    "        os.rename(txt_path, csv_path)\n",
    "\n",
    "print(\"Unzipping and renaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now my goal is to combine all this .csv files in one dataframe. I will creat new column for each row named `date`, where I will save date of operation.<br>I will save only data with `AFKS, AFLT, AGRO, ALRS, CBOM, GAZP, GLTR, GMKN, MAGN, MGNT, SBER, SBERP, TRNFP, VKCO, VTBR` constituents cause only them I will describe in this assignment.<br> At the same time I will remove all data that include `NaN` in price column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Filtered and cleaned data from all files combined and saved to: combined_order_logs.csv\n",
      "Combined data loaded into DataFrame successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Output file path\n",
    "output_folder = \"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\Unziped_Data\"\n",
    "\n",
    "save_to = \"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\combined_data\\\\combined_order_logs.csv\"\n",
    "\n",
    "# List of SECCODES to keep\n",
    "seccodes = [\"AFKS\", \"AFLT\", \"AGRO\", \"ALRS\", \"CBOM\", \"GAZP\", \"GLTR\", \"GMKN\", \n",
    "                  \"MAGN\", \"MGNT\", \"SBER\", \"SBERP\", \"TRNFP\", \"VKCO\", \"VTBR\"]\n",
    "\n",
    "# list of all .csv files in the folder\n",
    "files = [f for f in os.listdir(output_folder) if f.endswith('.csv')]\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    \n",
    "    date = file[-12:-4]\n",
    "    \n",
    "    # Read file in chunks\n",
    "    chunks = pd.read_csv(os.path.join(output_folder, file), chunksize=100000)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # I'll filter based on SECCODES\n",
    "        filtered_chunk = chunk[chunk['SECCODE'].isin(seccodes)]\n",
    "        \n",
    "        # Drop rows with NaN values in any column\n",
    "        cleaned_chunk = filtered_chunk.dropna(subset=['PRICE'])\n",
    "        \n",
    "        \n",
    "        cleaned_chunk['date'] = date\n",
    "        \n",
    "        # Append or create the combined file\n",
    "        mode = 'w' if i == 0 and chunk.index[0] == 0 else 'a'  # write for the 1st chunk, append for others\n",
    "        header = i == 0 and chunk.index[0] == 0  # write header only for the 1st chunk\n",
    "        cleaned_chunk.to_csv(save_to, mode=mode, header=header, index=False)\n",
    "\n",
    "print(f\"\\n\\n\\nFiltered and cleaned data from all files combined and saved to: combined_order_logs.csv\")\n",
    "\n",
    "# Load the entire combined file into a DataFrame\n",
    "combined_data = pd.read_csv(save_to, index_col=False)\n",
    "print(\"Combined data loaded into DataFrame successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of my dataframe: (68322575, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECCODE</th>\n",
       "      <th>BUYSELL</th>\n",
       "      <th>TIME</th>\n",
       "      <th>TRADENO</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>B</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>10</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>B</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.42</td>\n",
       "      <td>10</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>S</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151.00</td>\n",
       "      <td>2570</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>S</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>10</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>B</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>10</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>S</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>10</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>B</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>50</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>S</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>50</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>B</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>10</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GAZP</td>\n",
       "      <td>S</td>\n",
       "      <td>100000005313</td>\n",
       "      <td>8.521259e+09</td>\n",
       "      <td>167.83</td>\n",
       "      <td>10</td>\n",
       "      <td>20231002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SECCODE BUYSELL          TIME       TRADENO   PRICE  VOLUME      date\n",
       "0    GAZP       B  100000005313  8.521259e+09  167.83      10  20231002\n",
       "1    GAZP       B  100000005313           NaN  179.42      10  20231002\n",
       "2    GAZP       S  100000005313           NaN  151.00    2570  20231002\n",
       "3    GAZP       S  100000005313  8.521259e+09  167.83      10  20231002\n",
       "4    GAZP       B  100000005313  8.521259e+09  167.83      10  20231002\n",
       "5    GAZP       S  100000005313  8.521259e+09  167.83      10  20231002\n",
       "6    GAZP       B  100000005313  8.521259e+09  167.83      50  20231002\n",
       "7    GAZP       S  100000005313  8.521259e+09  167.83      50  20231002\n",
       "8    GAZP       B  100000005313  8.521259e+09  167.83      10  20231002\n",
       "9    GAZP       S  100000005313  8.521259e+09  167.83      10  20231002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\combined_data\\\\combined_order_logs.csv\", index_col=False)\n",
    "print(f\"Size of my dataframe: {combined_data.shape}\")\n",
    "\n",
    "combined_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end I can say that we've stored data only with constituents that we need and without any Nan values in price column and save it to `combined_order_logs.csv`. <br> Now we have: 68322575 numbers of rows in our dataframe. Lets see how we can clean our data more<br>We can remove all rows were TRADENO - Nan cause it's number of сделки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've done. Size of new dataframe: (18317572, 7)\n",
      "\n",
      "We've removed 73.190% of data for TRADENO criteria\n"
     ]
    }
   ],
   "source": [
    "filtered_data = combined_data.dropna(subset=['TRADENO'])\n",
    "\n",
    "print(f\"We've done. Size of new dataframe: {filtered_data.shape}\\n\\nWe've removed {100 - ((filtered_data.shape[0] / combined_data.shape[0]) * 100):.3f}% of data for TRADENO criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18317572 entries, 0 to 68322574\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   SECCODE  object \n",
      " 1   BUYSELL  object \n",
      " 2   TIME     int64  \n",
      " 3   TRADENO  float64\n",
      " 4   PRICE    float64\n",
      " 5   VOLUME   int64  \n",
      " 6   date     int64  \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've learned from pdf. We have Time in the next format: HHMMSSZZZXXX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\1625286355.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['hour'] = filtered_data['TIME'].astype(str).str[:2].astype(int)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\1625286355.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['minute'] = filtered_data['TIME'].astype(str).str[2:4].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've done. Size of new dataframe: (18255996, 7)\n",
      "\n",
      "We've removed 0.336% of data for time criteria\n"
     ]
    }
   ],
   "source": [
    "#create hour and minute columns from the TIME column\n",
    "filtered_data['hour'] = filtered_data['TIME'].astype(str).str[:2].astype(int) \n",
    "filtered_data['minute'] = filtered_data['TIME'].astype(str).str[2:4].astype(int)\n",
    "\n",
    "# filter 9:50 - 18:50  \n",
    "filtered_data_1 = filtered_data[\n",
    "    ((filtered_data['hour'] == 9) & (filtered_data['minute'] >= 50)) |\n",
    "    ((filtered_data['hour'] > 9) & (filtered_data['hour'] < 18)) |   \n",
    "    ((filtered_data['hour'] == 18) & (filtered_data['minute'] <= 50)) \n",
    "]\n",
    "\n",
    "# drop 'hour' and 'minute' columns\n",
    "filtered_data_1 = filtered_data_1.drop(columns=['hour', 'minute'])\n",
    "\n",
    "print(f\"We've done. Size of new dataframe: {filtered_data_1.shape}\\n\\nWe've removed {100 - ((filtered_data_1.shape[0] / filtered_data.shape[0]) * 100):.3f}% of data for time criteria\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's\n",
    " remove all duplicates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've done. Size of new dataframe: (18255996, 7)\n"
     ]
    }
   ],
   "source": [
    "filtered_data_1 = filtered_data_1.drop_duplicates()\n",
    "\n",
    "print(f\"We've done. Size of new dataframe: {filtered_data_1.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we don't have any duplicates<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECCODE: 0\n",
      "BUYSELL: 0\n",
      "TIME: 0\n",
      "VOLUME: 0\n",
      "date: 0\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"SECCODE\",\"BUYSELL\",\"TIME\",\"VOLUME\",\"date\"]\n",
    "\n",
    "for name in column_names:\n",
    "    nan_counts = filtered_data_1[name].isna().sum()\n",
    "    print(f'{name}: {nan_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECCODE: 15 unique values\n",
      "BUYSELL: 2 unique values\n",
      "date: 22 unique values\n"
     ]
    }
   ],
   "source": [
    "# let me cheack how many unique values do we have\n",
    "# should be SECCODE = 15, BUYSELL = 2, date = 22\n",
    "object_features = ['SECCODE', 'BUYSELL', 'date']\n",
    "\n",
    "for column in object_features:\n",
    "    print(f\"{column}: {filtered_data_1[column].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18255996, 7)\n",
      "We've done. Size of new dataframe: (18255996, 7)\n"
     ]
    }
   ],
   "source": [
    "# remove rows where PRICE has = 0\n",
    "preprocessed_df = filtered_data_1[filtered_data_1['PRICE'] != 0]\n",
    "print(preprocessed_df.shape)\n",
    "\n",
    "# Display the shape of the resulting DataFrame\n",
    "print(f\"We've done. Size of new dataframe: {preprocessed_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can remove duplicate trades where the same transaction is recorded twice: once as `Buy` and once as `Sell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've done. Size of new dataframe: (9127998, 7)\n",
      "\n",
      "We've removed 50.000% of data for `transaction duplicates` criteria\n"
     ]
    }
   ],
   "source": [
    "processed_df = preprocessed_df.drop_duplicates(subset=['SECCODE', 'TIME', 'TRADENO', 'PRICE', 'VOLUME', 'date'])\n",
    "\n",
    "print(f\"We've done. Size of new dataframe: {processed_df.shape}\\n\\nWe've removed {(((preprocessed_df.shape[0] - processed_df.shape[0])/ preprocessed_df.shape[0]) * 100):.3f}% of data for `transaction duplicates` criteria\")\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file if needed\n",
    "processed_df.to_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\Processed_data\\\\processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've done with preprocessing of data. \n",
    "### Next step is to:\n",
    "1. Calculate average daily ruble volume\n",
    "2. Daily realized volatility (the square root of the sum of all squared log-differences of prices during the day)\n",
    "3. Number of transactions\n",
    "4. Median ruble size of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Average Daily Volume  Daily Volatility  Average Transactions  \\\n",
      "GAZP           3.527903e+09          0.015113          57816.727273   \n",
      "AGRO           1.328405e+09          0.042442          31938.409091   \n",
      "ALRS           1.155141e+09          0.027572          35250.636364   \n",
      "AFLT           2.762012e+08          0.025703          10220.863636   \n",
      "SBER           8.199113e+09          0.012620          87203.000000   \n",
      "AFKS           3.307517e+08          0.016860           9987.500000   \n",
      "GMKN           1.511901e+09          0.016184          16858.772727   \n",
      "MGNT           1.749113e+09          0.019400          22706.681818   \n",
      "MAGN           5.997891e+08          0.021033          22077.863636   \n",
      "SBERP          6.654762e+08          0.013609          12259.090909   \n",
      "VTBR           2.038567e+09          0.031034          56356.000000   \n",
      "TRNFP          1.685541e+09          0.016451           5432.636364   \n",
      "GLTR           1.252516e+08          0.025501           9372.409091   \n",
      "CBOM           1.530522e+08          0.019212           6468.727273   \n",
      "VKCO           3.479165e+09          0.101175          85139.125000   \n",
      "\n",
      "       Median Ruble Size  \n",
      "GAZP           10163.400  \n",
      "AGRO            7704.600  \n",
      "ALRS            4455.600  \n",
      "AFLT            2032.750  \n",
      "SBER           13006.750  \n",
      "AFKS            6985.600  \n",
      "GMKN           32600.000  \n",
      "MGNT           22217.000  \n",
      "MAGN            2656.125  \n",
      "SBERP          10862.000  \n",
      "VTBR            2483.250  \n",
      "TRNFP         143975.000  \n",
      "GLTR            2559.200  \n",
      "CBOM           11308.800  \n",
      "VKCO            4571.150  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_volatility = grouped.apply(realized_volatility).mean()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\4143237582.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\Processed_data\\\\processed_data.csv\")\n",
    "\n",
    "# it's our seccode\n",
    "seccodes = data['SECCODE'].unique()\n",
    "\n",
    "# dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Loop through each SECCODE\n",
    "for code in seccodes:\n",
    "    # filter data\n",
    "    stock_data = data[data['SECCODE'] == code]\n",
    "    \n",
    "    # group by date for daily calculations\n",
    "    grouped = stock_data.groupby('date')\n",
    "    \n",
    "    # average daily ruble volume\n",
    "    buy_avg_daily_volume = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum()).mean()\n",
    "    \n",
    "    # daily realized volatility\n",
    "    def realized_volatility(group):\n",
    "        log_returns = np.log(group['PRICE']).diff()\n",
    "        return np.sqrt((log_returns ** 2).sum())\n",
    "    \n",
    "    buy_volatility = grouped.apply(realized_volatility).mean()\n",
    "    \n",
    "    # number of transactions\n",
    "    buy_transactions = grouped.size().mean()\n",
    "    \n",
    "    # median ruble size of transactions\n",
    "    buy_median_ruble_size = grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).median()).median()\n",
    "    \n",
    "    # Store results for this SECCODE\n",
    "    results[code] = {\n",
    "        \"Average Daily Volume\": buy_avg_daily_volume,\n",
    "        \"Daily Volatility\": buy_volatility,\n",
    "        \"Average Transactions\": buy_transactions,\n",
    "        \"Median Ruble Size\": buy_median_ruble_size,\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(results_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df.to_csv(\"4th_task.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done with this task. Result u can see in `4th_task.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "1. Which stock was the most/least volatile during the period under investigation?\n",
    "2. Which stock had the largest/lowest share volume? ruble volume?\n",
    "3. Which stock had the largest/lowest number of transactions?\n",
    "4. Which stock had the largest/lowest number of quotes changes?\n",
    "5. On which day of the week stocks had the largest/lowest volume in the sample?\n",
    "6. Which stock had the highest/lowest ruble bid-ask spread? percentage bid-ask\n",
    "spread?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\574761241.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  volatility = data.groupby('SECCODE').apply(realized_volatility)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\574761241.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ruble_volume = data.groupby('SECCODE').apply(lambda x: (x['PRICE'] * x['VOLUME']).sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Most volatile stock: VKCO, Least volatile stock: SBER\n",
      "2. Largest share volume: VTBR, Lowest share volume: TRNFP\n",
      "   Largest ruble volume: SBER, Lowest ruble volume: GLTR\n",
      "3. Largest transactions: SBER, Lowest transactions: TRNFP\n",
      "4. Largest quote changes: AGRO, Lowest quote changes: TRNFP\n",
      "5. Day with largest volume: Monday, Day with lowest volume: Thursday\n",
      "6. ASK and BID columns not found. Cannot calculate spreads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4476\\574761241.py:58: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  weekday_ruble_volume = weekday_grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum())\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    metrics = pd.read_csv(\"4th_task.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Metrics file not found. Recalculating metrics from the dataset.\")\n",
    "    metrics = None\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\Innopolis_courses\\\\Financy_project\\\\data\\\\Processed_data\\\\processed_data.csv\") \n",
    "\n",
    "# adding weekday column for weekly calculations\n",
    "data['weekday'] = pd.to_datetime(data['date'], format='%Y%m%d').dt.day_name()\n",
    "\n",
    "# function to calculate realized volatility\n",
    "def realized_volatility(group):\n",
    "    log_returns = np.log(group['PRICE']).diff()\n",
    "    return np.sqrt((log_returns ** 2).sum())\n",
    "\n",
    "# most/least volatile stock\n",
    "if metrics is not None and 'Realized Volatility' in metrics.columns:\n",
    "    most_volatile = metrics.loc[metrics['Realized Volatility'].idxmax(), 'SECCODE']\n",
    "    least_volatile = metrics.loc[metrics['Realized Volatility'].idxmin(), 'SECCODE']\n",
    "else:\n",
    "    volatility = data.groupby('SECCODE').apply(realized_volatility)\n",
    "    most_volatile = volatility.idxmax()\n",
    "    least_volatile = volatility.idxmin()\n",
    "\n",
    "# largest/lowest share volume and ruble volume\n",
    "if metrics is not None and 'Combined Avg Daily Volume' in metrics.columns:\n",
    "    largest_share_volume = metrics.loc[metrics['Combined Avg Daily Volume'].idxmax(), 'SECCODE']\n",
    "    lowest_share_volume = metrics.loc[metrics['Combined Avg Daily Volume'].idxmin(), 'SECCODE']\n",
    "    # with using ruble volume metrics\n",
    "    largest_ruble_volume = metrics.loc[metrics['Buy Avg Daily Volume'].idxmax(), 'SECCODE']\n",
    "    lowest_ruble_volume = metrics.loc[metrics['Sell Avg Daily Volume'].idxmin(), 'SECCODE']\n",
    "else:\n",
    "    share_volume = data.groupby('SECCODE')['VOLUME'].sum()\n",
    "    ruble_volume = data.groupby('SECCODE').apply(lambda x: (x['PRICE'] * x['VOLUME']).sum())\n",
    "    largest_share_volume = share_volume.idxmax()\n",
    "    lowest_share_volume = share_volume.idxmin()\n",
    "    largest_ruble_volume = ruble_volume.idxmax()\n",
    "    lowest_ruble_volume = ruble_volume.idxmin()\n",
    "\n",
    "# largest/lowest number of transactions ---\n",
    "if metrics is not None and 'Combined Avg Transactions' in metrics.columns:\n",
    "    largest_transactions = metrics.loc[metrics['Combined Avg Transactions'].idxmax(), 'SECCODE']\n",
    "    lowest_transactions = metrics.loc[metrics['Combined Avg Transactions'].idxmin(), 'SECCODE']\n",
    "else:\n",
    "    transactions = data.groupby('SECCODE').size()\n",
    "    largest_transactions = transactions.idxmax()\n",
    "    lowest_transactions = transactions.idxmin()\n",
    "\n",
    "# largest/lowest number of quote changes\n",
    "quote_changes = data.groupby('SECCODE')['PRICE'].nunique()\n",
    "largest_quote_changes = quote_changes.idxmax()\n",
    "lowest_quote_changes = quote_changes.idxmin()\n",
    "\n",
    "#day of the week with largest/lowest volume\n",
    "weekday_grouped = data.groupby('weekday')\n",
    "weekday_ruble_volume = weekday_grouped.apply(lambda x: (x['PRICE'] * x['VOLUME']).sum())\n",
    "largest_volume_day = weekday_ruble_volume.idxmax()\n",
    "lowest_volume_day = weekday_ruble_volume.idxmin()\n",
    "\n",
    "# highest/lowest ruble bid-ask spread and percentage spread\n",
    "if 'ASK' in data.columns and 'BID' in data.columns:\n",
    "    data['ruble_spread'] = data['ASK'] - data['BID']\n",
    "    data['percentage_spread'] = (data['ASK'] - data['BID']) / ((data['ASK'] + data['BID']) / 2) * 100\n",
    "    spread_grouped = data.groupby('SECCODE')\n",
    "    highest_ruble_spread = spread_grouped['ruble_spread'].mean().idxmax()\n",
    "    lowest_ruble_spread = spread_grouped['ruble_spread'].mean().idxmin()\n",
    "    highest_percentage_spread = spread_grouped['percentage_spread'].mean().idxmax()\n",
    "    lowest_percentage_spread = spread_grouped['percentage_spread'].mean().idxmin()\n",
    "else:\n",
    "    highest_ruble_spread = None\n",
    "    lowest_ruble_spread = None\n",
    "    highest_percentage_spread = None\n",
    "    lowest_percentage_spread = None\n",
    "\n",
    "\n",
    "print(f\"1. Most volatile stock: {most_volatile}, Least volatile stock: {least_volatile}\")\n",
    "print(f\"2. Largest share volume: {largest_share_volume}, Lowest share volume: {lowest_share_volume}\")\n",
    "print(f\"   Largest ruble volume: {largest_ruble_volume}, Lowest ruble volume: {lowest_ruble_volume}\")\n",
    "print(f\"3. Largest transactions: {largest_transactions}, Lowest transactions: {lowest_transactions}\")\n",
    "print(f\"4. Largest quote changes: {largest_quote_changes}, Lowest quote changes: {lowest_quote_changes}\")\n",
    "print(f\"5. Day with largest volume: {largest_volume_day}, Day with lowest volume: {lowest_volume_day}\")\n",
    "if 'ASK' in data.columns and 'BID' in data.columns:\n",
    "    print(f\"6. Highest ruble spread: {highest_ruble_spread}, Lowest ruble spread: {lowest_ruble_spread}\")\n",
    "    print(f\"   Highest percentage spread: {highest_percentage_spread}, Lowest percentage spread: {lowest_percentage_spread}\")\n",
    "else:\n",
    "    print(\"6. ASK and BID columns not found. Cannot calculate spreads.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next task:<br>  \n",
    "Examine empirically what is the relationship between the number of trades and the\n",
    "number of quotes changes? For example, plot the log of the number of trades against\n",
    "the log of the number of quotes changes calculated for each stock and each trading day.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
